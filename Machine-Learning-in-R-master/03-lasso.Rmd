# OLS and lasso

## Load packages

Load any packages we need for this analysis.

```{r load_packages}
library(glmnet)
library(ggplot2)
```

## Ordinary least squares

Below is an refresher of ordinary least squares linear (OLS) regression that predicts age using the other variables as predictors. 

```{r}
# Load data we created in 02-preprocessing.Rmd.
# Objects: task_reg, task_class
load("data/preprocessed.RData")

# Pull out data for easier analysis.
train_x = task_reg$data[task_reg$train_rows, task_reg$covariates]
train_y = task_reg$data[task_reg$train_rows, task_reg$outcome]

test_x = task_reg$data[-task_reg$train_rows, task_reg$covariates]
test_y = task_reg$data[-task_reg$train_rows, task_reg$outcome]

# Look at ages of first 20 individuals
head(train_y, n = 20)

# Look at features for the corresponding first 6 individuals
head(train_x)

# Fit the regression model; lm() will automatically add a temporary intercept column
fit = lm(train_y ~ ., data = train_x)

# View the output
summary(fit) 

# Predict outcome for the test data
predicted = predict(fit, test_x)

# 8. Calculate mean-squared error
(mse_reg = mean((test_y - predicted)^2))

# Root mean-squared error
sqrt(mse_reg)
```

## Lasso

Did you notice the warning message after the `predict()`?

> Warning message:
  ...
  prediction from a rank-deficient fit may be misleading

[Here we have a rank-deficient matrix](https://stats.stackexchange.com/questions/35071/what-is-rank-deficiency-and-how-to-deal-with-it).

However, we can use lasso to try and remove some of the non-associated features from the model. Because glmnet expects a matrix of predictors, use `as.matrix` to convert it from a data frame to a matrix. 

```{r}
lasso = cv.glmnet(as.matrix(train_x), train_y, family = "gaussian", alpha = 1)
```

Visualize the distribution of log(lamba) vs mean-squared error.   
```{r}
plot(lasso)

# Generate our own version, but plot lambda (not on log scale) vs. RMSE.
qplot(lasso$lambda, sqrt(lasso$cvm)) + theme_minimal()

```

> NOTE: when log(lamba) is equal to 0 that means lambda is equal to 1. In this graph, the far right side is overpenalized, as the model is emphasizing the beta coefficients being small. As log(lambda) becomes increasingly negative, lambda is correspondingly closer to zero and we are approaching the OLS solution. 

```{r}
# And here is a plot of log(lambda) vs lambda.
qplot(log(lasso$lambda), lasso$lambda) + theme_minimal()
```

Show plot of different lambda values: 
```{r}
plot(lasso$glmnet.fit, xvar = "lambda", label = TRUE)
```

Show the lambda that results in the minimum estimated mean-squared error (MSE):
```{r}
lasso$lambda.min
```

Show higher lambda within [one standard error](https://stats.stackexchange.com/questions/80268/empirical-justification-for-the-one-standard-error-rule-when-using-cross-validat) of performance of the minimum
```{r}
lasso$lambda.1se

# Log scale versions:
log(c("log_min" = lasso$lambda.min, "log_1se" = lasso$lambda.1se))
```

Look at the coefficients
```{r}
(coef_1se = coef(lasso, s = "lambda.1se"))
```

Look at the coefficients for lambda.min
```{r}
(coef_min = coef(lasso, s = "lambda.min"))

# Compare side-by-side
cbind(as.matrix(coef_1se), as.matrix(coef_min))
```

Predict on the test set
```{r}
predictions = predict(lasso, newx = as.matrix(test_x),
                      s = lasso$lambda.1se)

# How far off were we, based on absolute error?
rounded_errors = round(abs(test_y - predictions))
table(rounded_errors)

# Group the absolute error into 4 bins.
grouped_errors = round(abs(test_y - predictions) / 5)
grouped_errors[grouped_errors > 2] = 3
table(grouped_errors)

# 4 categories of accuracy
how_close = factor(grouped_errors, labels = c("very close", "close", "meh", "far"))
table(rounded_errors, how_close)

# Scatter plot of actual vs. predicted
qplot(test_y, predictions, color = how_close) + theme_minimal()
```

Calculate MSE and RMSE:
```{r}
# Calculate mean-squared error.
mean((predictions - test_y)^2)

# Calculate root mean-squared error.
sqrt(mean((predictions - test_y)^2))
```

**Big questions:**  
1. What are the statistical assupmtions for OLS regression?  
2. What are the statistical assumptions for Lasso?  
3. What is a rank-deficient matrix?  

### Challenge 1
Construct a lasso to predict the "Petal.Width" variable from the iris dataset. What predictors are most strongly associated with values for "Petal.Width"? 